{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b4b812-8b78-43fd-8694-30fa136c1549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install pandas-gbq google-cloud-bigquery requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9ca52a-94b8-4714-a591-f96ce4df00e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas_gbq\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550027e0-ef88-47f2-8e33-d4db7ce89f5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Google Cloud Project\n",
    "PROJECT_ID = \"adcz-adoki-poc\"\n",
    "LOCATION_ID = \"europe-west1\"\n",
    "\n",
    "print(f\"Project '{PROJECT_ID}' on a Location '{LOCATION_ID}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a101024f-7c6c-4237-95dd-46b661d98fa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use local computing power\n",
    "dataset_id = \"demo_real_estate\"\n",
    "table_id = \"property\"\n",
    "\n",
    "sql = f\"SELECT property_id, address_latitude, address_longitude FROM {dataset_id}.{table_id} WHERE address_latitude > 0 LIMIT 10\"\n",
    "df_property = pandas_gbq.read_gbq(sql, project_id=PROJECT_ID, location=LOCATION_ID)\n",
    "\n",
    "display(df_property)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0dfbb8-6c21-4a9b-89d3-6b640a48a662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coords_df = df_property[['address_latitude', 'address_longitude']].dropna().drop_duplicates().rename(columns={\n",
    "    'address_latitude': 'lat',\n",
    "    'address_longitude': 'lon'\n",
    "})\n",
    "\n",
    "hourly_params = [\n",
    "    'temperature_2m',\n",
    "    'relative_humidity_2m',\n",
    "    'rain',\n",
    "    'snowfall',\n",
    "    'snow_depth',\n",
    "    'windspeed_10m'\n",
    " ]\n",
    "\n",
    "results = []\n",
    "BASE_URL = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "target_date = (datetime.now() - timedelta(days=2)).date()\n",
    "\n",
    "for row in coords_df.itertuples(index=False):\n",
    "    lat, lon = row.lat, row.lon\n",
    "    params = {\n",
    "        'latitude': lat,\n",
    "        'longitude': lon,\n",
    "        'start_date': target_date,\n",
    "        'end_date': target_date,\n",
    "        'hourly': ','.join(hourly_params),\n",
    "        'timezone': 'Europe/Prague'\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(BASE_URL, params=params, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        hourly = data.get('hourly', {})\n",
    "        times = hourly.get('time', [])\n",
    "\n",
    "        for i, ts in enumerate(times):\n",
    "            try:\n",
    "                date_part, time_part = ts.split('T')\n",
    "                hour_part = int(time_part.split(':')[0])\n",
    "                record = {\n",
    "                    'lat': lat,\n",
    "                    'lon': lon,\n",
    "                    'date': date_part,\n",
    "                    'hour': hour_part,\n",
    "                    'temperature': hourly.get('temperature_2m', [None]*len(times))[i],\n",
    "                    'relative_humidity': hourly.get('relative_humidity_2m', [None]*len(times))[i],\n",
    "                    'rain': hourly.get('rain', [None]*len(times))[i],\n",
    "                    'wind_speed': hourly.get('windspeed_10m', [None]*len(times))[i],\n",
    "                    'snow_depth': None\n",
    "                }\n",
    "                # Prefer snow_depth, fallback to snowfall\n",
    "                snow_depth_val = hourly.get('snow_depth', [None]*len(times))[i]\n",
    "                snowfall_val = hourly.get('snowfall', [None]*len(times))[i]\n",
    "                record['snow_depth'] = snow_depth_val if snow_depth_val is not None else snowfall_val\n",
    "\n",
    "                results.append(record)\n",
    "            except Exception:\n",
    "                continue\n",
    "        time.sleep(0.15)  # Be gentle to the API\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching weather for {lat},{lon}: {e}\")\n",
    "\n",
    "weather_df = pd.DataFrame(results)\n",
    "\n",
    "prop_coord_map = df_property[['property_id', 'address_latitude', 'address_longitude']].dropna()\n",
    "\n",
    "# Merge weather data with property IDs\n",
    "merged = weather_df.merge(\n",
    "    prop_coord_map,\n",
    "    left_on=['lat', 'lon'],\n",
    "    right_on=['address_latitude', 'address_longitude'],\n",
    "    how='left'\n",
    " )\n",
    "\n",
    "final_weather_df = merged[['property_id', 'date', 'hour', 'temperature', 'relative_humidity', 'rain', 'snow_depth', 'wind_speed']].copy()\n",
    "\n",
    "if not final_weather_df.empty:\n",
    "    final_weather_df['hour'] = final_weather_df['hour'].astype(int)\n",
    "\n",
    "print(f\"Total hourly weather records: {len(final_weather_df)}\")\n",
    "display(final_weather_df.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e54d0a-e71f-4b6f-8765-f68a580372fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process_id = os.getenv(\"AIP_EXECUTION_ID\", \"exception\")\n",
    "now = datetime.utcnow()\n",
    "final_weather_df[\"ins_dt\"] = now\n",
    "final_weather_df[\"ins_process_id\"] = process_id\n",
    "final_weather_df[\"upd_dt\"] = now\n",
    "final_weather_df[\"upd_process_id\"] = process_id\n",
    "final_weather_df[\"del_flag\"] = False\n",
    "\n",
    "if \"date\" in final_weather_df.columns:\n",
    "    final_weather_df[\"date\"] = pd.to_datetime(final_weather_df[\"date\"]).dt.date\n",
    "\n",
    "# Verify the DataFrame before loading\n",
    "print(\"--- Verifying DataFrame before loading ---\")\n",
    "final_weather_df.info()\n",
    "print(\"\\n\")\n",
    "display(final_weather_df.head())\n",
    "print(\"----------------------------------------\")\n",
    "\n",
    "\n",
    "# Import to BigQuery\n",
    "project_id = \"adcz-adoki-poc\"\n",
    "dataset_id = \"demo_real_estate\"\n",
    "table_id = \"property_weather\"\n",
    "full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n",
    "# Define the schema to ensure data types match the target table\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"property_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"date\", \"DATE\"),\n",
    "    bigquery.SchemaField(\"hour\", \"INT64\"),\n",
    "    bigquery.SchemaField(\"temperature\", \"FLOAT64\"),\n",
    "    bigquery.SchemaField(\"relative_humidity\", \"FLOAT64\"),\n",
    "    bigquery.SchemaField(\"rain\", \"FLOAT64\"),\n",
    "    bigquery.SchemaField(\"snow_depth\", \"FLOAT64\"),\n",
    "    bigquery.SchemaField(\"wind_speed\", \"FLOAT64\"),\n",
    "    bigquery.SchemaField(\"ins_dt\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"ins_process_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"upd_dt\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"upd_process_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"del_flag\", \"BOOL\"),\n",
    "]\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=schema,\n",
    "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND\n",
    ")\n",
    "\n",
    "print(f\"\\nAttempting to load {len(final_weather_df)} rows into {full_table_id}...\")\n",
    "\n",
    "try:\n",
    "    job = client.load_table_from_dataframe(final_weather_df, full_table_id, job_config=job_config)\n",
    "    job.result()  # Wait for the job to complete.\n",
    "\n",
    "    if job.errors:\n",
    "        print(\"\\nBigQuery job failed with errors:\")\n",
    "        for error in job.errors:\n",
    "            print(f\"- {error['message']}\")\n",
    "    else:\n",
    "        table = client.get_table(full_table_id)\n",
    "        print(f\"\\nSuccessfully loaded {job.output_rows} rows.\")\n",
    "        print(f\"Table '{full_table_id}' now contains {table.num_rows} rows.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An exception occurred during the load job: {e}\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
